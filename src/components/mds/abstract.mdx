# Abstract

We present VirtualSqueeze, a novel framework that enables
pressure-driven visual and haptic interaction with deformable virtual objects in real time. Leveraging per-finger pressure data captured from a SenseGlove, our system maps individualized squeezing gestures to physically plausible object deformations. These deformations are computed using either high-fidelity simulations or
learned graph-based models and serve as conditioning signals for a
generative diffusion model (AnimateDiff), which synthesizes temporally coherent visual effects. To close the interaction loop, VirtualSqueeze delivers deformation-aware haptic feedback—including
resistive force and vibrotactile cues—computed from the simulated
physical state of the object. We evaluate the system through quantitative metrics and user studies, demonstrating its effectiveness in
reproducing realistic deformation dynamics, generating perceptually convincing visual sequences, and enhancing user immersion
through tactile feedback. While limitations remain in terms of object generalization and multi-view synthesis, our findings establish VirtualSqueeze as a compelling foundation for personalized,
pressure-based interaction in immersive virtual environments.